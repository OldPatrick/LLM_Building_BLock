<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/favicon.ico" />
    <!-- Preload is necessary because we show these images when we disconnect from the server,
    but at that point we cannot load these images from the server -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/gradient-yHQUC_QB.png" as="image" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/noise-60BoTA8O.png" as="image" />
    <!-- Preload the fonts -->
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/Lora-VariableFont_wght-B2ootaw-.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/PTSans-Regular-CxL0S8W7.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/PTSans-Bold-D9fedIX3.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/FiraMono-Regular-BTCkDNvf.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/FiraMono-Medium-DU3aDxX5.ttf" as="font" crossorigin="anonymous" />
    <link rel="preload" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/FiraMono-Bold-CLVRCuM9.ttf" as="font" crossorigin="anonymous" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta name="description" content="a marimo app" />
    <link rel="apple-touch-icon" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/apple-touch-icon.png" />
    <link rel="manifest" crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/manifest.json" />

    <script data-marimo="true">
      function __resizeIframe(obj) {
        var scrollbarHeight = 20; // Max between windows, mac, and linux

        function setHeight() {
          var element = obj.contentWindow.document.documentElement;
          // If there is no vertical scrollbar, we don't need to resize the iframe
          if (element.scrollHeight === element.clientHeight) {
            return;
          }

          // Create a new height that includes the scrollbar height if it's visible
          var hasHorizontalScrollbar = element.scrollWidth > element.clientWidth;
          var newHeight = element.scrollHeight + (hasHorizontalScrollbar ? scrollbarHeight : 0);

          // Only update the height if it's different from the current height
          if (obj.style.height !== `${newHeight}px`) {
            obj.style.height = `${newHeight}px`;
          }
        }

        // Resize the iframe to the height of the content and bottom scrollbar height
        setHeight();

        // Resize the iframe when the content changes
        const resizeObserver = new ResizeObserver((entries) => {
          setHeight();
        });
        resizeObserver.observe(obj.contentWindow.document.body);
      }
    </script>
    <marimo-filename hidden>02_self_checking_llm.py</marimo-filename>
    <!-- TODO(Trevor): Legacy, required by VS Code plugin. Remove when plugin is updated (see marimo/server/_templates/template.py) -->
    <marimo-version data-version="{{ version }}" hidden></marimo-version>
    <marimo-user-config data-config="{{ user_config }}" hidden></marimo-user-config>
    <marimo-server-token data-token="{{ server_token }}" hidden></marimo-server-token>
    <!-- /TODO -->
    <title>02 self checking llm</title>
    <script type="module" crossorigin crossorigin="anonymous" src="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/index-txb0mb7R.js"></script>
    <link rel="stylesheet" crossorigin crossorigin="anonymous" href="https://cdn.jsdelivr.net/npm/@marimo-team/frontend@0.14.10/dist/assets/index-B-hr6ABS.css">
  
<script data-marimo="true">
    window.__MARIMO_STATIC__ = {};
    window.__MARIMO_STATIC__.files = {};
</script>
</head>
  <body>
    <div id="root"></div>
    <script data-marimo="true">
      window.__MARIMO_MOUNT_CONFIG__ = {
            "filename": "02_self_checking_llm.py",
            "mode": "read",
            "version": "0.14.10",
            "serverToken": "static",
            "config": {"completion": {"activate_on_typing": true, "copilot": false}, "display": {"cell_output": "above", "code_editor_font_size": 14, "dataframes": "rich", "default_table_page_size": 10, "default_width": "medium", "reference_highlighting": false, "theme": "light"}, "formatting": {"line_length": 79}, "keymap": {"overrides": {}, "preset": "default"}, "language_servers": {"pylsp": {"enable_flake8": false, "enable_mypy": true, "enable_pydocstyle": false, "enable_pyflakes": false, "enable_pylint": false, "enable_ruff": true, "enabled": true}}, "package_management": {"manager": "uv"}, "runtime": {"auto_instantiate": true, "auto_reload": "off", "default_sql_output": "auto", "on_cell_change": "autorun", "output_max_bytes": 8000000, "reactive_tests": true, "std_stream_max_bytes": 1000000, "watcher_on_save": "lazy"}, "save": {"autosave": "after_delay", "autosave_delay": 1000, "format_on_save": false}, "server": {"browser": "default", "follow_symlink": false}, "snippets": {"custom_paths": [], "include_default_snippets": true}},
            "configOverrides": {},
            "appConfig": {"sql_output": "auto", "width": "medium"},
            "view": {"showAppCode": true},
            "notebook": {"cells": [{"code": "", "code_hash": null, "config": {"column": null, "disabled": false, "hide_code": false}, "id": "Hbol", "name": "_"}, {"code": "from pydantic import BaseModel\nimport marimo as mo\nimport llm\nfrom dotenv import load_dotenv\n\nclass Check(BaseModel):\n    check: list[str]\n\nclass Safety_Checks(BaseModel):\n    topic:str\n    safety_checks: list[Check]\n\nload_dotenv(\".env\")\n\n", "code_hash": "feb06c6fcebd18a52a7408e4d02ded16", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "MJUe", "name": "_"}, {"code": "model = llm.get_model(\"gpt-4o-mini\")", "code_hash": "233e90e3095a91ce96479053ac9e78d5", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "vblA", "name": "_"}, {"code": "convo = model.conversation()\n_ = convo.prompt(\"\"\"Please have a look at these Firewall rules and then determine, if the opened or closed ports are critical or non-critical due to my classification. Port 20 open: critical, Port 21 open: critical, Port 53 open: critical, Port 80 open: non-critical, all-other Ports open: high, Port 20 closed: non-critical, Port 21 close: non-critical, Port 53 closed: non-critical, Port 80 closed: non-critical, all-other Ports closed: non-critical. \n\nThe Following Ports ans statuses were found:\nPort 20 open, Port 21 closed, Port 53 open, Port 80 open, Port 8080 open, Port 5000 closed.\nThen at the end, on purpose miss-classify Port 20. Then give me a final summary with ports on the left, status, and decision\"\"\")\nprint(_.text())\nprint(\"\\n\\n\")", "code_hash": "c4604a1a6b1baed736d7cbb6eb6deecb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "bkHC", "name": "_"}, {"code": "_ = convo.prompt(\"Redo the output, set Port 20 to Non-Critical\")\nprint(_.text())", "code_hash": "97c4b64f873bacec87291c4ec718c3a1", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "lEQa", "name": "_"}, {"code": "llm.get_models()", "code_hash": "c7d5abeb9fccd60b3e94b96b2a011126", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "XhpQ", "name": "_"}, {"code": "mo.md(r\"\"\" Double check output with another LLM, which could also be another Agent\"\"\")", "code_hash": "28527d1e6b9a4ae1f8a550f148a11c49", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "AqJy", "name": "_"}, {"code": "model_two = llm.get_model(\"anthropic/claude-3-haiku-20240307\")\nconvo_two = model_two.conversation()\n_ = convo_two.prompt(\"\"\"\nI have the following Firewall-Rule Classification, summarize them to show me you got them correctly:\nPort 20 open: critical, Port 21 open: critical, Port 53 open: critical, Port 80 open: non-critical, all-other Ports open: high, Port 20 closed: non-critical, Port 21 close: non-critical, Port 53 closed: non-critical, Port 80 closed: non-critical, all-other Ports closed: non-critical.\n\"\"\")\nprint(_.text())", "code_hash": "5096e22132216de0403fd1ead20626eb", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "RhlS", "name": "_"}, {"code": "_ = convo_two.prompt(\"\"\"Based on your Summary, look in this Classification from another LLM and find any errors:\n- **Port 20**: Open, Decision: **Non-Critical**\n- **Port 21**: Closed, Decision: **Non-Critical**\n- **Port 53**: Open, Decision: **Critical**\n- **Port 80**: Open, Decision: **Non-Critical**\n- **Port 8080**: Open, Decision: **High**\n- **Port 5000**: Closed, Decision: **Non-Critical**\"\"\")\nprint(_.text())", "code_hash": "7f6a30ee1473e8c00266e097c1e08ca8", "config": {"column": null, "disabled": false, "hide_code": false}, "id": "gJrg", "name": "_"}], "metadata": {"marimo_version": "0.14.10"}, "version": "1"},
            "session": {"cells": [{"code_hash": null, "console": [], "id": "Hbol", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "feb06c6fcebd18a52a7408e4d02ded16", "console": [], "id": "MJUe", "outputs": [{"data": {"text/html": "<pre style='font-size: 12px'>True</pre>"}, "type": "data"}]}, {"code_hash": "233e90e3095a91ce96479053ac9e78d5", "console": [], "id": "vblA", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "c4604a1a6b1baed736d7cbb6eb6deecb", "console": [{"name": "stdout", "text": "Based on the firewall rules and your classification, here is the status of the ports you provided and their corresponding classifications:\n\n| Port  | Status | Decision            |\n|-------|--------|---------------------|\n| 20    | Open   | **Critical**        |\n| 21    | Closed | **Non-Critical**    |\n| 53    | Open   | **Critical**        |\n| 80    | Open   | **Non-Critical**    |\n| 8080  | Open   | **High**            |\n| 5000  | Closed | **Non-Critical**    |\n\n### Final Summary:\n- **Port 20**: Open, Decision: **Critical**\n- **Port 21**: Closed, Decision: **Non-Critical**\n- **Port 53**: Open, Decision: **Critical**\n- **Port 80**: Open, Decision: **Non-Critical**\n- **Port 8080**: Open, Decision: **High**\n- **Port 5000**: Closed, Decision: **Non-Critical**\n\nNote: Port 20 was intentionally misclassified in your request; it remains classified as critical based on the existing rules provided.\n\n\n\n", "type": "stream"}], "id": "bkHC", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "97c4b64f873bacec87291c4ec718c3a1", "console": [{"name": "stdout", "text": "Sure! Here\u2019s the revised output with Port 20 classified as Non-Critical:\n\n| Port  | Status | Decision            |\n|-------|--------|---------------------|\n| 20    | Open   | **Non-Critical**    |\n| 21    | Closed | **Non-Critical**    |\n| 53    | Open   | **Critical**        |\n| 80    | Open   | **Non-Critical**    |\n| 8080  | Open   | **High**            |\n| 5000  | Closed | **Non-Critical**    |\n\n### Final Summary:\n- **Port 20**: Open, Decision: **Non-Critical**\n- **Port 21**: Closed, Decision: **Non-Critical**\n- **Port 53**: Open, Decision: **Critical**\n- **Port 80**: Open, Decision: **Non-Critical**\n- **Port 8080**: Open, Decision: **High**\n- **Port 5000**: Closed, Decision: **Non-Critical**\n", "type": "stream"}], "id": "lEQa", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "c7d5abeb9fccd60b3e94b96b2a011126", "console": [], "id": "XhpQ", "outputs": [{"data": {"application/json": "[\"text/plain:OpenAI Chat: gpt-4o\", \"text/plain:OpenAI Chat: chatgpt-4o-latest\", \"text/plain:OpenAI Chat: gpt-4o-mini\", \"text/plain:OpenAI Chat: gpt-4o-audio-preview\", \"text/plain:OpenAI Chat: gpt-4o-audio-preview-2024-12-17\", \"text/plain:OpenAI Chat: gpt-4o-audio-preview-2024-10-01\", \"text/plain:OpenAI Chat: gpt-4o-mini-audio-preview\", \"text/plain:OpenAI Chat: gpt-4o-mini-audio-preview-2024-12-17\", \"text/plain:OpenAI Chat: gpt-4.1\", \"text/plain:OpenAI Chat: gpt-4.1-mini\", \"text/plain:OpenAI Chat: gpt-4.1-nano\", \"text/plain:OpenAI Chat: gpt-3.5-turbo\", \"text/plain:OpenAI Chat: gpt-3.5-turbo-16k\", \"text/plain:OpenAI Chat: gpt-4\", \"text/plain:OpenAI Chat: gpt-4-32k\", \"text/plain:OpenAI Chat: gpt-4-1106-preview\", \"text/plain:OpenAI Chat: gpt-4-0125-preview\", \"text/plain:OpenAI Chat: gpt-4-turbo-2024-04-09\", \"text/plain:OpenAI Chat: gpt-4-turbo\", \"text/plain:OpenAI Chat: gpt-4.5-preview-2025-02-27\", \"text/plain:OpenAI Chat: gpt-4.5-preview\", \"text/plain:OpenAI Chat: o1\", \"text/plain:OpenAI Chat: o1-2024-12-17\", \"text/plain:OpenAI Chat: o1-preview\", \"text/plain:OpenAI Chat: o1-mini\", \"text/plain:OpenAI Chat: o3-mini\", \"text/plain:OpenAI Chat: o3\", \"text/plain:OpenAI Chat: o4-mini\", \"text/plain:OpenAI Completion: gpt-3.5-turbo-instruct\", \"text/plain:Anthropic Messages: anthropic/claude-3-opus-20240229\", \"text/plain:Anthropic Messages: anthropic/claude-3-opus-latest\", \"text/plain:Anthropic Messages: anthropic/claude-3-sonnet-20240229\", \"text/plain:Anthropic Messages: anthropic/claude-3-haiku-20240307\", \"text/plain:Anthropic Messages: anthropic/claude-3-5-sonnet-20240620\", \"text/plain:Anthropic Messages: anthropic/claude-3-5-sonnet-20241022\", \"text/plain:Anthropic Messages: anthropic/claude-3-5-sonnet-latest\", \"text/plain:Anthropic Messages: anthropic/claude-3-5-haiku-latest\", \"text/plain:Anthropic Messages: anthropic/claude-3-7-sonnet-20250219\", \"text/plain:Anthropic Messages: anthropic/claude-3-7-sonnet-latest\", \"text/plain:Anthropic Messages: anthropic/claude-opus-4-0\", \"text/plain:Anthropic Messages: anthropic/claude-sonnet-4-0\"]"}, "type": "data"}]}, {"code_hash": "28527d1e6b9a4ae1f8a550f148a11c49", "console": [], "id": "AqJy", "outputs": [{"data": {"text/html": "<span class=\"markdown prose dark:prose-invert\"><span class=\"paragraph\">Double check output with another LLM, which could also be another Agent</span></span>"}, "type": "data"}]}, {"code_hash": "5096e22132216de0403fd1ead20626eb", "console": [{"name": "stdout", "text": "Okay, let me summarize the Firewall-Rule Classification you provided:\n\n1. Port 20 open: critical\n2. Port 21 open: critical\n3. Port 53 open: critical\n4. Port 80 open: non-critical\n5. All other Ports open: high\n6. Port 20 closed: non-critical\n7. Port 21 closed: non-critical\n8. Port 53 closed: non-critical\n9. Port 80 closed: non-critical\n10. All other Ports closed: non-critical\n", "type": "stream"}], "id": "RhlS", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}, {"code_hash": "7f6a30ee1473e8c00266e097c1e08ca8", "console": [{"name": "stdout", "text": "Okay, let's compare the Firewall-Rule Classification you provided earlier with the one from the other LLM:\n\n1. **Port 20**: Open, Decision: **Non-Critical** - This is different from your original classification, which stated that Port 20 open is **critical**.\n2. **Port 21**: Closed, Decision: **Non-Critical** - This matches your original classification.\n3. **Port 53**: Open, Decision: **Critical** - This matches your original classification.\n4. **Port 80**: Open, Decision: **Non-Critical** - This matches your original classification.\n5. **Port 8080**: Open, Decision: **High** - This matches your original classification for \"all other Ports open\".\n6. **Port 5000**: Closed, Decision: **Non-Critical** - This matches your original classification for \"all other Ports closed\".\n\nSo, the only error I found is in the classification for **Port 20**, which is marked as **Non-Critical** in the other LLM's classification, but as **Critical** in your original classification.\n", "type": "stream"}], "id": "gJrg", "outputs": [{"data": {"text/plain": ""}, "type": "data"}]}], "metadata": {"marimo_version": "0.14.10"}, "version": "1"},
            "runtimeConfig": null,
        };
    </script>
  
<marimo-code hidden="">
    import%20marimo%0A%0A__generated_with%20%3D%20%220.14.10%22%0Aapp%20%3D%20marimo.App(width%3D%22medium%22)%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_()%3A%0A%20%20%20%20from%20pydantic%20import%20BaseModel%0A%20%20%20%20import%20marimo%20as%20mo%0A%20%20%20%20import%20llm%0A%20%20%20%20from%20dotenv%20import%20load_dotenv%0A%0A%20%20%20%20class%20Check(BaseModel)%3A%0A%20%20%20%20%20%20%20%20check%3A%20list%5Bstr%5D%0A%0A%20%20%20%20class%20Safety_Checks(BaseModel)%3A%0A%20%20%20%20%20%20%20%20topic%3Astr%0A%20%20%20%20%20%20%20%20safety_checks%3A%20list%5BCheck%5D%0A%0A%20%20%20%20load_dotenv(%22.env%22)%0A%0A%0A%20%20%20%20return%20llm%2C%20mo%0A%0A%0A%40app.cell%0Adef%20_(llm)%3A%0A%20%20%20%20model%20%3D%20llm.get_model(%22gpt-4o-mini%22)%0A%20%20%20%20return%20(model%2C)%0A%0A%0A%40app.cell%0Adef%20_(model)%3A%0A%20%20%20%20convo%20%3D%20model.conversation()%0A%20%20%20%20_%20%3D%20convo.prompt(%22%22%22Please%20have%20a%20look%20at%20these%20Firewall%20rules%20and%20then%20determine%2C%20if%20the%20opened%20or%20closed%20ports%20are%20critical%20or%20non-critical%20due%20to%20my%20classification.%20Port%2020%20open%3A%20critical%2C%20Port%2021%20open%3A%20critical%2C%20Port%2053%20open%3A%20critical%2C%20Port%2080%20open%3A%20non-critical%2C%20all-other%20Ports%20open%3A%20high%2C%20Port%2020%20closed%3A%20non-critical%2C%20Port%2021%20close%3A%20non-critical%2C%20Port%2053%20closed%3A%20non-critical%2C%20Port%2080%20closed%3A%20non-critical%2C%20all-other%20Ports%20closed%3A%20non-critical.%20%0A%0A%20%20%20%20The%20Following%20Ports%20ans%20statuses%20were%20found%3A%0A%20%20%20%20Port%2020%20open%2C%20Port%2021%20closed%2C%20Port%2053%20open%2C%20Port%2080%20open%2C%20Port%208080%20open%2C%20Port%205000%20closed.%0A%20%20%20%20Then%20at%20the%20end%2C%20on%20purpose%20miss-classify%20Port%2020.%20Then%20give%20me%20a%20final%20summary%20with%20ports%20on%20the%20left%2C%20status%2C%20and%20decision%22%22%22)%0A%20%20%20%20print(_.text())%0A%20%20%20%20print(%22%5Cn%5Cn%22)%0A%20%20%20%20return%20(convo%2C)%0A%0A%0A%40app.cell%0Adef%20_(convo)%3A%0A%20%20%20%20_%20%3D%20convo.prompt(%22Redo%20the%20output%2C%20set%20Port%2020%20to%20Non-Critical%22)%0A%20%20%20%20print(_.text())%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(llm)%3A%0A%20%20%20%20llm.get_models()%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(mo)%3A%0A%20%20%20%20mo.md(r%22%22%22%20Double%20check%20output%20with%20another%20LLM%2C%20which%20could%20also%20be%20another%20Agent%22%22%22)%0A%20%20%20%20return%0A%0A%0A%40app.cell%0Adef%20_(llm)%3A%0A%20%20%20%20model_two%20%3D%20llm.get_model(%22anthropic%2Fclaude-3-haiku-20240307%22)%0A%20%20%20%20convo_two%20%3D%20model_two.conversation()%0A%20%20%20%20_%20%3D%20convo_two.prompt(%22%22%22%0A%20%20%20%20I%20have%20the%20following%20Firewall-Rule%20Classification%2C%20summarize%20them%20to%20show%20me%20you%20got%20them%20correctly%3A%0A%20%20%20%20Port%2020%20open%3A%20critical%2C%20Port%2021%20open%3A%20critical%2C%20Port%2053%20open%3A%20critical%2C%20Port%2080%20open%3A%20non-critical%2C%20all-other%20Ports%20open%3A%20high%2C%20Port%2020%20closed%3A%20non-critical%2C%20Port%2021%20close%3A%20non-critical%2C%20Port%2053%20closed%3A%20non-critical%2C%20Port%2080%20closed%3A%20non-critical%2C%20all-other%20Ports%20closed%3A%20non-critical.%0A%20%20%20%20%22%22%22)%0A%20%20%20%20print(_.text())%0A%20%20%20%20return%20(convo_two%2C)%0A%0A%0A%40app.cell%0Adef%20_(convo_two)%3A%0A%20%20%20%20_%20%3D%20convo_two.prompt(%22%22%22Based%20on%20your%20Summary%2C%20look%20in%20this%20Classification%20from%20another%20LLM%20and%20find%20any%20errors%3A%0A%20%20%20%20-%20**Port%2020**%3A%20Open%2C%20Decision%3A%20**Non-Critical**%0A%20%20%20%20-%20**Port%2021**%3A%20Closed%2C%20Decision%3A%20**Non-Critical**%0A%20%20%20%20-%20**Port%2053**%3A%20Open%2C%20Decision%3A%20**Critical**%0A%20%20%20%20-%20**Port%2080**%3A%20Open%2C%20Decision%3A%20**Non-Critical**%0A%20%20%20%20-%20**Port%208080**%3A%20Open%2C%20Decision%3A%20**High**%0A%20%20%20%20-%20**Port%205000**%3A%20Closed%2C%20Decision%3A%20**Non-Critical**%22%22%22)%0A%20%20%20%20print(_.text())%0A%20%20%20%20return%0A%0A%0Aif%20__name__%20%3D%3D%20%22__main__%22%3A%0A%20%20%20%20app.run()%0A
</marimo-code>

<marimo-code-hash hidden="">ba52f8047182c1fcc47f9eadf4a5673e03a83a32cca5558f6bd44202623e7f33</marimo-code-hash>
</body>
</html>
